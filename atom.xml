<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ezeli&#39;s Blog</title>
  
  <subtitle>人生在勤，不索何获？</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ezeli.github.io/"/>
  <updated>2021-02-01T12:44:04.843Z</updated>
  <id>https://ezeli.github.io/</id>
  
  <author>
    <name>Ezeli</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Attention-Related Image Captioning (4)</title>
    <link href="https://ezeli.github.io/2021/02/01/Attention-Related-Image-Captioning-4/"/>
    <id>https://ezeli.github.io/2021/02/01/Attention-Related-Image-Captioning-4/</id>
    <published>2021-02-01T12:40:32.000Z</published>
    <updated>2021-02-01T12:44:04.843Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-exploring-and-distilling-cross-modal-information-for-image-captioning-ijcai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-exploring-and-distilling-cross-modal-information-for-image-captioning-ijcai2019&quot;&gt;&lt;/a&gt; 一、Exploring and Distilling Cross-Modal Information for Image Captioning, IJCAI2019&lt;/h2&gt;
&lt;p&gt;作者认为深层次的图片理解需要与图片区域相关的视觉注意力和与对应属性相关的语义注意力，所以作者从跨模态（包括视觉和文本属性）的视角提出了Global-and-Local Information Exploring-and-Distilling (GLIED) 方法，如下图所示，左边是基础模型，右边是利用全局和局部的源信息的完整模型。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>元学习应用论文</title>
    <link href="https://ezeli.github.io/2021/01/27/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E8%AE%BA%E6%96%87/"/>
    <id>https://ezeli.github.io/2021/01/27/元学习应用论文/</id>
    <published>2021-01-27T14:36:15.000Z</published>
    <updated>2021-01-27T14:41:48.839Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-meta-learning-for-image-captioning-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-meta-learning-for-image-captioning-aaai2019&quot;&gt;&lt;/a&gt; 一、Meta Learning for Image Captioning, AAAI2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2019%20Meta%20Learning%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;使用强化学习训练模型，可能会使模型与奖励函数过拟合，造成奖励黑客行为（reward hacking），也就是说虽然奖励函数的得分提高了但是模型的实际性能是降低的，这是因为奖励函数不能非常正确的表达出生成描述的质量，一些错误的表示可能反而有很高的奖励，尤其是对描述的命题内容和独特性的奖励。比如，使用CIDEr优化倾向于让句子以“介词+a”结尾，因为CIDEr评价指标会惩罚太短的句子并给常见的短语小的权重，因此，当模型生成短描述时RL会添加一些小权重但是常见的短语来避免惩罚。SPICE指标认为不正常结尾是不匹配的对象-关系对，会惩罚这种现象，但是，SPICE有自己的奖励黑客问题，因为它不惩罚场景图中的重复元组。从技术上讲，很难设计一个完美的评价指标，能够考虑到预期目标的每一个方面。&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
      <category term="Face Recognition" scheme="https://ezeli.github.io/tags/Face-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Active Learning for Image Captioning</title>
    <link href="https://ezeli.github.io/2021/01/04/Active-Learning-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2021/01/04/Active-Learning-for-Image-Captioning/</id>
    <published>2021-01-04T02:38:20.000Z</published>
    <updated>2021-01-04T02:46:57.745Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-structural-semantic-adversarial-active-learning-for-image-captioning-acmmm2020-oral&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-structural-semantic-adversarial-active-learning-for-image-captioning-acmmm2020-oral&quot;&gt;&lt;/a&gt; 一、Structural Semantic Adversarial Active Learning for Image Captioning, ACMMM2020 oral&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ACMMM2020%20oral%20Structural%20Semantic%20Adversarial%20Active%20Learning%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;标注图片描述的成本非常高。&lt;/p&gt;
&lt;h3 id=&quot;2-方法&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#2-方法&quot;&gt;&lt;/a&gt; 2、方法&lt;/h3&gt;
&lt;h4 id=&quot;1active-learning&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1active-learning&quot;&gt;&lt;/a&gt; 1）Active learning&lt;/h4&gt;
&lt;p&gt;引入主动学习（Active learning）方法来从未标注的图片集中选取最具代表性的图片来进行标注，再用于训练captioning模型。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Active Learning" scheme="https://ezeli.github.io/tags/Active-Learning/"/>
    
  </entry>
  
  <entry>
    <title>元学习基础论文</title>
    <link href="https://ezeli.github.io/2020/12/28/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E8%AE%BA%E6%96%87/"/>
    <id>https://ezeli.github.io/2020/12/28/元学习基础论文/</id>
    <published>2020-12-28T07:46:05.000Z</published>
    <updated>2020-12-28T08:03:47.294Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-learning-to-learn-by-gradient-descent-by-gradient-descent-nips2016&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-learning-to-learn-by-gradient-descent-by-gradient-descent-nips2016&quot;&gt;&lt;/a&gt; 一、Learning to learn by gradient descent by gradient descent, NIPS2016&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;在机器学习中，学习得到的特征取代了手工设计的特征，并取得了巨大的成功。但是，优化算法依然是手工设计的，针对不同的任务需要设计不同的更新规则，比如深度学习中常用的momentum、RMSprop、ADAM等。这篇论文中，作者将优化算法参数化，使用LSTM来作为优化器，通过学习的方式得到更新规则，也就是论文题目：通过梯度下降来学习如何通过梯度下降学习。这样就不需要针对不同的任务设计不同的优化算法，而只需要让LSTM优化器自己学习如何优化模型的参数即可。&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Image Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/12/16/Detailed-Image-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/12/16/Detailed-Image-Captioning-2/</id>
    <published>2020-12-16T13:20:09.000Z</published>
    <updated>2020-12-16T13:39:56.784Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-say-as-you-wish-fine-grained-control-of-image-caption-generation-with-abstract-scene-graphs-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-say-as-you-wish-fine-grained-control-of-image-caption-generation-with-abstract-scene-graphs-cvpr2020&quot;&gt;&lt;/a&gt; 一、Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs, CVPR2020&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;现有的方法无法根据用户的意图细粒度的控制生成的描述，比如控制以什么层次的细节程度来描述图片中的物体、以及那些属性和关系应该包含在描述中等等。只存在一些粗粒度的控制，比如控制描述的风格、描述哪个物体、描述图片那个区域等。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
  </entry>
  
  <entry>
    <title>Stylized Image Captioning</title>
    <link href="https://ezeli.github.io/2020/11/02/Stylized-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/11/02/Stylized-Image-Captioning/</id>
    <published>2020-11-02T01:05:57.000Z</published>
    <updated>2020-11-02T01:16:23.812Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-unsupervised-stylish-image-description-generation-via-domain-layer-norm-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-unsupervised-stylish-image-description-generation-via-domain-layer-norm-aaai2019&quot;&gt;&lt;/a&gt; 一、Unsupervised Stylish Image Description Generation via Domain Layer Norm, AAAI2019&lt;/h2&gt;
&lt;p&gt;作者提出了一种无监督风格化描述生成模型，能够以配对的无风格数据和没有配对的风格化语料进行训练，并且它使用户能够通过插入特定样式的参数来生成各种风格描述，灵活地将新的样式包含到现有模型中。论文将配对的无风格描述视为源域数据，将未配对的风格化语料视为目标域数据，最主要的贡献就是展示了只需要对layer normalization的参数进行调整就能从源域和目标域区分出语言风格，并将这种机制称为Domain Layer Normalization (DLN)，结构图如下：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
      <category term="Stylized Image Captioning" scheme="https://ezeli.github.io/tags/Stylized-Image-Captioning/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Image Captioning</title>
    <link href="https://ezeli.github.io/2020/10/18/Detailed-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/10/18/Detailed-Image-Captioning/</id>
    <published>2020-10-18T07:28:03.000Z</published>
    <updated>2020-10-18T07:36:50.260Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-compare-and-reweight-distinctive-image-captioning-using-similar-images-sets-eccv2020-oral&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-compare-and-reweight-distinctive-image-captioning-using-similar-images-sets-eccv2020-oral&quot;&gt;&lt;/a&gt; 一、Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets, ECCV2020 oral&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ECCV2020%20oral%20Compare%20and%20Reweight%20Distinctive%20Image%20Captioning%20Using%20Similar%20Images%20Sets.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;1）现在的方法生成的句子可以准确的描述图片，但是对于相似的图片，生成的句子是通用的，缺乏独特性。如下图所示，CIDErBtw是作者提出的一种衡量描述独特性的指标，值越小表示越独特，对于两个相似的图片，人们标注的描述具有很好的独特性，但是baseline模型生成的描述就是相同的，而作者的方法生成的描述则具有不错的独特性。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>元学习基础知识</title>
    <link href="https://ezeli.github.io/2020/10/13/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>https://ezeli.github.io/2020/10/13/元学习基础知识/</id>
    <published>2020-10-13T07:40:01.000Z</published>
    <updated>2020-10-13T07:56:13.399Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;From &lt;a href=&quot;https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;amp;index=32&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2019%20Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;introduction&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#introduction&quot;&gt;&lt;/a&gt; Introduction&lt;/h2&gt;
&lt;p&gt;Meta learning = Learn to learn&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
  </entry>
  
  <entry>
    <title>Scene Graph-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/10/04/Scene-Graph-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/10/04/Scene-Graph-Related-Image-Captioning/</id>
    <published>2020-10-04T15:37:35.000Z</published>
    <updated>2020-10-04T15:57:31.950Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-auto-encoding-scene-graphs-for-image-captioning-cvpr2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-auto-encoding-scene-graphs-for-image-captioning-cvpr2019&quot;&gt;&lt;/a&gt; 一、Auto-Encoding Scene Graphs for Image Captioning, CVPR2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2019%20Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;当我们看到“person on bike”，我们会很自然的把“on”替换成“ride”并且推理出“person riding bike on a road”即使“road”并没有出现，我们人类能够通过这种inductive bias（归纳能力、常识、先验知识）来进行单词搭配和语境推理。因此，探索这种推理可以让模型不过度拟合于数据集偏差而专注于推理。之前的工作中，当我们将一个看不见的图像场景输入到模型中时，通常会得到一个简单而没有价值的关于显著物体的标题，例如“there is a dog on the floor”，这比目标检测得到的结果好不了多少。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
      <category term="Unsupervised Learning" scheme="https://ezeli.github.io/tags/Unsupervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improve Optimization Method for Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/09/27/Improve-Optimization-Method-for-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/09/27/Improve-Optimization-Method-for-Captioning-2/</id>
    <published>2020-09-27T12:37:21.000Z</published>
    <updated>2020-09-27T13:06:33.568Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-reinforcing-an-image-caption-generator-using-off-line-human-feedback-aaai2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-reinforcing-an-image-caption-generator-using-off-line-human-feedback-aaai2020&quot;&gt;&lt;/a&gt; 一、Reinforcing an Image Caption Generator Using Off-Line Human Feedback, AAAI2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2020%20Reinforcing%20an%20Image%20Caption%20Generator%20Using%20Off-Line%20Human%20Feedback.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;之前模型主要是通过最大似然估计（MLE）以及使用CIDEr等手工设计的评价指标作为奖励函数的强化学习的方式进行优化，但是这些优化方法是受限的，我们人类对模型生成描述的质量评估可能并不高。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Video Captioning" scheme="https://ezeli.github.io/tags/Video-Captioning/"/>
    
  </entry>
  
  <entry>
    <title>Image Captioning with Image-Text Matching Model</title>
    <link href="https://ezeli.github.io/2020/09/21/Image-Captioning-with-Image-Text-Matching-Model/"/>
    <id>https://ezeli.github.io/2020/09/21/Image-Captioning-with-Image-Text-Matching-Model/</id>
    <published>2020-09-21T11:47:07.000Z</published>
    <updated>2020-09-27T12:46:48.997Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-more-grounded-image-captioning-by-distilling-image-text-matching-model-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-more-grounded-image-captioning-by-distilling-image-text-matching-model-cvpr2020&quot;&gt;&lt;/a&gt; 一、More Grounded Image Captioning by Distilling Image-Text Matching Model, CVPR2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2020%20More%20Grounded%20Image%20Captioning%20by%20Distilling%20Image-Text%20Matching%20Model.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;注意力机制的目的是为了让模型在生成对应单词时将注意力集中到正确的物体上，这种能力被称为grounded image captioning，但是现有模型的定位精度远远不能令人满意，并且如果为了提高定位精度而收集单词-区域对齐数据作为强监督信息，代价是很昂贵的。因此，作者提出Part-of-Speech enhanced image-text matching model（POS-SCAN）作为一种知识提取方法来规范模型的注意力，为模型提供一种弱的定位监督信息。所谓的“弱”是由于POS-SCAN只依赖于图片-文本对齐，而不需要昂贵的单词-区域对齐。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Image-Text Matching" scheme="https://ezeli.github.io/tags/Image-Text-Matching/"/>
    
  </entry>
  
  <entry>
    <title>Mining Ground Truth Information for Image Captioning</title>
    <link href="https://ezeli.github.io/2020/09/13/Mining-Ground-Truth-Information-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/09/13/Mining-Ground-Truth-Information-for-Image-Captioning/</id>
    <published>2020-09-13T12:23:53.000Z</published>
    <updated>2020-09-13T12:46:25.422Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-generating-diverse-and-descriptive-image-captions-using-visual-paraphrases-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-generating-diverse-and-descriptive-image-captions-using-visual-paraphrases-iccv2019&quot;&gt;&lt;/a&gt; 一、Generating Diverse and Descriptive Image Captions Using Visual Paraphrases, ICCV2019&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;计算机更喜欢生成流畅正确但是简单模糊的描述，因为这样的描述更加“安全”，能够描述图片中显著的区域，但是会忽略细节。描述相同图片的不同的句子被称为visual paraphrases，之前的方法会忽略它们之间的联系，直接把它们作为不同的样本，而作者探索了它们之间的关系并使用一系列打分函数选择了一些visual paraphrase对（Ci，Cj），打分函数衡量了visual paraphrases在某种特征（比如多样性）上的差异，而Cj在这种特征上比Ci更“复杂”，比如Cj比Ci更丰富，训练时，模型首先更加视觉特征生成初始描述Ci，之后再融合视觉特征和Ci提供的文本特征生成更加丰富多样的Cj。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/07/12/Transformer-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/07/12/Transformer-Related-Image-Captioning/</id>
    <published>2020-07-12T15:32:10.000Z</published>
    <updated>2020-07-12T15:55:23.275Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-entangled-transformer-for-image-captioning-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-entangled-transformer-for-image-captioning-iccv2019&quot;&gt;&lt;/a&gt; 一、Entangled Transformer for Image Captioning, ICCV2019&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;之前的注意力机制主要分为两种：视觉注意力和语义注意力，对于视觉注意力来说，能够探索图片底层的特征或者高层的显著对象特征，但是由于视觉和语言之间存在差异，描述中不是每个词都有对应的视觉信号，特别是对于一些抽象概念词和复杂的关系词。对于语义注意力来说，能够直接利用高层的语义信息，但是由于RNN的长期依赖问题，很难记忆很多步之前的输入信息，尤其是最初的视觉输入，导致模型倾向于生成一些高频短语而不考虑视觉信息。并且之前的方法要不只考虑一种注意力，要不分开考虑两种注意力然后在进行融合，这样不能同时探索两种注意力之间的互补关系。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (3)</title>
    <link href="https://ezeli.github.io/2020/04/26/Attention-Related-Image-Captioning-3/"/>
    <id>https://ezeli.github.io/2020/04/26/Attention-Related-Image-Captioning-3/</id>
    <published>2020-04-26T14:50:11.000Z</published>
    <updated>2020-04-26T15:03:14.713Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-hierarchical-attention-network-for-image-captioning-aaai-2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-hierarchical-attention-network-for-image-captioning-aaai-2019&quot;&gt;&lt;/a&gt; 一、Hierarchical Attention Network for Image Captioning, AAAI 2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2019%20Hierarchical%20Attention%20Network%20for%20Image%20Captioning.ppt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;这篇论文认为现有的注意力机制只关注单层特征，比如低层的空间特征或者高层的文本特征，但是描述语句中不同的单词和不同层次的特征有关，比如颜色单词可以从低层特征预测，量词可以从中层特征预测。所以作者提出了一个&lt;br&gt;
Hierarchical Attention Network (HAN)，能够同时考虑多层特征，如下图所示：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>Improve Optimization Method for Image Captioning</title>
    <link href="https://ezeli.github.io/2020/04/19/Improve-Optimization-Method-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/04/19/Improve-Optimization-Method-for-Image-Captioning/</id>
    <published>2020-04-19T14:27:04.000Z</published>
    <updated>2020-04-20T08:26:29.523Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-improving-image-captioning-with-conditional-generative-adversarial-nets-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-improving-image-captioning-with-conditional-generative-adversarial-nets-aaai2019&quot;&gt;&lt;/a&gt; 一、Improving Image Captioning with Conditional Generative Adversarial Nets, AAAI2019&lt;/h2&gt;
&lt;p&gt;这篇论文的思路很简单，就是引入GAN来提高模型生成描述的质量，这种方法是通用的，可以加入到所有的基于强化学习的模型中。模型的整体框架如下所示：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Cross-domain Image Captioning</title>
    <link href="https://ezeli.github.io/2020/04/12/Cross-domain-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/04/12/Cross-domain-Image-Captioning/</id>
    <published>2020-04-12T12:11:43.000Z</published>
    <updated>2020-04-20T08:26:29.513Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在配对的图片-文本描述数据集（下面称为源域）上，描述生成模型已经取得了不错的效果，但是由于配对数据获取是费时费力的，代价昂贵，所以产生了跨域图片文本描述任务，其目的是利用源域数据，给未配对的数据集（下面称为目标域）生成图片描述，但是由于不同的数据集之间存在域偏移，直接使用在源域上训练好的模型给目标域图片生成的描述，质量往往很差，此时，就可以使用迁移学习方法解决该问题。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Transfer Learning" scheme="https://ezeli.github.io/tags/Transfer-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/04/06/Attention-Related-Image-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/04/06/Attention-Related-Image-Captioning-2/</id>
    <published>2020-04-06T10:58:57.000Z</published>
    <updated>2020-04-20T08:26:29.495Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-reflective-decoding-network-for-image-captioning-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-reflective-decoding-network-for-image-captioning-iccv2019&quot;&gt;&lt;/a&gt; 一、Reflective Decoding Network for Image Captioning, ICCV2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ICCV2019%20Reflective%20Decoding%20Network%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;这篇论文认为当图片的内容比较复杂时传统的LSTM很难处理长期依赖问题，往往更关注于相对比较近的单词而忽略比较远的问题，如下图所示，“bridge”对“river”有很大的提示作用，但是它们之间相差6个单词，对于传统的LSTM来说很容易忽略这个。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/03/29/Attention-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/03/29/Attention-Related-Image-Captioning/</id>
    <published>2020-03-29T12:42:45.000Z</published>
    <updated>2020-04-21T04:30:32.397Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-show-attend-and-tell-neural-image-caption-generation-with-visual-attention-icml2015&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-show-attend-and-tell-neural-image-caption-generation-with-visual-attention-icml2015&quot;&gt;&lt;/a&gt; 一、Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML2015&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;我们人类观察图片时，不是将整个图片压缩成一个静态表示，而是通过注意力机制根据需要动态调节所观察的区域，当图像中存在大量杂乱时，这一点尤其重要。因此，作者参考人类的视觉系统，在编解码框架的基础上引入了注意力机制，使得模型在生成描述时可以动态关注图片的不同区域。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>(ICCV2019) Towards Unsupervised Image Captioning with Shared Multimodal Embeddings</title>
    <link href="https://ezeli.github.io/2019/10/01/ICCV2019-Towards-Unsupervised-Image-Captioning-with-Shared-Multimodal-Embeddings/"/>
    <id>https://ezeli.github.io/2019/10/01/ICCV2019-Towards-Unsupervised-Image-Captioning-with-Shared-Multimodal-Embeddings/</id>
    <published>2019-10-01T14:45:04.000Z</published>
    <updated>2020-04-20T08:26:29.474Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.09317&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文链接&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;一-主要思想&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-主要思想&quot;&gt;&lt;/a&gt; 一、主要思想&lt;/h2&gt;
&lt;p&gt;文章首先提出了传统方法的问题：给图片标注描述信息很繁琐、昂贵，并且这些人工描述也都很短而且是重复的；当前的方法生成的描述受限于图片类别数和并不完美的验证指标，因此很难扩展到数据集以外的图片中。本文使用无监督的方式对独立的图像和文本进行建模，避免了带有人为偏见的而且昂贵的人工标注，当然，语料和图片并不是完全无关的，比如试图用经济学文本语料来描述图片是不合理的。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="DL" scheme="https://ezeli.github.io/tags/DL/"/>
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
      <category term="Seq2Seq" scheme="https://ezeli.github.io/tags/Seq2Seq/"/>
    
  </entry>
  
  <entry>
    <title>(CVPR2019) Unsupervised Image Captioning</title>
    <link href="https://ezeli.github.io/2019/09/20/CVPR2019-Unsupervised-Image-Captioning/"/>
    <id>https://ezeli.github.io/2019/09/20/CVPR2019-Unsupervised-Image-Captioning/</id>
    <published>2019-09-20T13:28:31.000Z</published>
    <updated>2020-04-21T04:07:53.428Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.10787&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文链接&lt;/a&gt; | &lt;a href=&quot;https://github.com/fengyang0317/unsupervised_captioning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码链接&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;一-主要思想&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-主要思想&quot;&gt;&lt;/a&gt; 一、主要思想&lt;/h2&gt;
&lt;p&gt;作者首先总结了以往的Image Caption的方法，将这些方法分成以下几类：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="DL" scheme="https://ezeli.github.io/tags/DL/"/>
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
  </entry>
  
</feed>
