<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ezeli&#39;s Blog</title>
  
  <subtitle>人生在勤，不索何获？</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ezeli.github.io/"/>
  <updated>2020-11-02T01:16:23.812Z</updated>
  <id>https://ezeli.github.io/</id>
  
  <author>
    <name>Ezeli</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Stylized Image Captioning</title>
    <link href="https://ezeli.github.io/2020/11/02/Stylized-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/11/02/Stylized-Image-Captioning/</id>
    <published>2020-11-02T01:05:57.000Z</published>
    <updated>2020-11-02T01:16:23.812Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-unsupervised-stylish-image-description-generation-via-domain-layer-norm-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-unsupervised-stylish-image-description-generation-via-domain-layer-norm-aaai2019&quot;&gt;&lt;/a&gt; 一、Unsupervised Stylish Image Description Generation via Domain Layer Norm, AAAI2019&lt;/h2&gt;
&lt;p&gt;作者提出了一种无监督风格化描述生成模型，能够以配对的无风格数据和没有配对的风格化语料进行训练，并且它使用户能够通过插入特定样式的参数来生成各种风格描述，灵活地将新的样式包含到现有模型中。论文将配对的无风格描述视为源域数据，将未配对的风格化语料视为目标域数据，最主要的贡献就是展示了只需要对layer normalization的参数进行调整就能从源域和目标域区分出语言风格，并将这种机制称为Domain Layer Normalization (DLN)，结构图如下：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
      <category term="Stylized Image Captioning" scheme="https://ezeli.github.io/tags/Stylized-Image-Captioning/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Image Captioning</title>
    <link href="https://ezeli.github.io/2020/10/18/Detailed-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/10/18/Detailed-Image-Captioning/</id>
    <published>2020-10-18T07:28:03.000Z</published>
    <updated>2020-10-18T07:36:50.260Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-compare-and-reweight-distinctive-image-captioning-using-similar-images-sets-eccv2020-oral&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-compare-and-reweight-distinctive-image-captioning-using-similar-images-sets-eccv2020-oral&quot;&gt;&lt;/a&gt; 一、Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets, ECCV2020 oral&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ECCV2020%20oral%20Compare%20and%20Reweight%20Distinctive%20Image%20Captioning%20Using%20Similar%20Images%20Sets.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;1）现在的方法生成的句子可以准确的描述图片，但是对于相似的图片，生成的句子是通用的，缺乏独特性。如下图所示，CIDErBtw是作者提出的一种衡量描述独特性的指标，值越小表示越独特，对于两个相似的图片，人们标注的描述具有很好的独特性，但是baseline模型生成的描述就是相同的，而作者的方法生成的描述则具有不错的独特性。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>元学习基础知识</title>
    <link href="https://ezeli.github.io/2020/10/13/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>https://ezeli.github.io/2020/10/13/元学习基础知识/</id>
    <published>2020-10-13T07:40:01.000Z</published>
    <updated>2020-10-13T07:56:13.399Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;From &lt;a href=&quot;https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;amp;index=32&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2019%20Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;introduction&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#introduction&quot;&gt;&lt;/a&gt; Introduction&lt;/h2&gt;
&lt;p&gt;Meta learning = Learn to learn&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
  </entry>
  
  <entry>
    <title>Scene Graph-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/10/04/Scene-Graph-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/10/04/Scene-Graph-Related-Image-Captioning/</id>
    <published>2020-10-04T15:37:35.000Z</published>
    <updated>2020-10-04T15:57:31.950Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-auto-encoding-scene-graphs-for-image-captioning-cvpr2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-auto-encoding-scene-graphs-for-image-captioning-cvpr2019&quot;&gt;&lt;/a&gt; 一、Auto-Encoding Scene Graphs for Image Captioning, CVPR2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2019%20Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;当我们看到“person on bike”，我们会很自然的把“on”替换成“ride”并且推理出“person riding bike on a road”即使“road”并没有出现，我们人类能够通过这种inductive bias（归纳能力、常识、先验知识）来进行单词搭配和语境推理。因此，探索这种推理可以让模型不过度拟合于数据集偏差而专注于推理。之前的工作中，当我们将一个看不见的图像场景输入到模型中时，通常会得到一个简单而没有价值的关于显著物体的标题，例如“there is a dog on the floor”，这比目标检测得到的结果好不了多少。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
      <category term="Unsupervised Learning" scheme="https://ezeli.github.io/tags/Unsupervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improve Optimization Method for Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/09/27/Improve-Optimization-Method-for-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/09/27/Improve-Optimization-Method-for-Captioning-2/</id>
    <published>2020-09-27T12:37:21.000Z</published>
    <updated>2020-09-27T13:06:33.568Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-reinforcing-an-image-caption-generator-using-off-line-human-feedback-aaai2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-reinforcing-an-image-caption-generator-using-off-line-human-feedback-aaai2020&quot;&gt;&lt;/a&gt; 一、Reinforcing an Image Caption Generator Using Off-Line Human Feedback, AAAI2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2020%20Reinforcing%20an%20Image%20Caption%20Generator%20Using%20Off-Line%20Human%20Feedback.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;之前模型主要是通过最大似然估计（MLE）以及使用CIDEr等手工设计的评价指标作为奖励函数的强化学习的方式进行优化，但是这些优化方法是受限的，我们人类对模型生成描述的质量评估可能并不高。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Video Captioning" scheme="https://ezeli.github.io/tags/Video-Captioning/"/>
    
  </entry>
  
  <entry>
    <title>Image Captioning with Image-Text Matching Model</title>
    <link href="https://ezeli.github.io/2020/09/21/Image-Captioning-with-Image-Text-Matching-Model/"/>
    <id>https://ezeli.github.io/2020/09/21/Image-Captioning-with-Image-Text-Matching-Model/</id>
    <published>2020-09-21T11:47:07.000Z</published>
    <updated>2020-09-27T12:46:48.997Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-more-grounded-image-captioning-by-distilling-image-text-matching-model-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-more-grounded-image-captioning-by-distilling-image-text-matching-model-cvpr2020&quot;&gt;&lt;/a&gt; 一、More Grounded Image Captioning by Distilling Image-Text Matching Model, CVPR2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2020%20More%20Grounded%20Image%20Captioning%20by%20Distilling%20Image-Text%20Matching%20Model.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;注意力机制的目的是为了让模型在生成对应单词时将注意力集中到正确的物体上，这种能力被称为grounded image captioning，但是现有模型的定位精度远远不能令人满意，并且如果为了提高定位精度而收集单词-区域对齐数据作为强监督信息，代价是很昂贵的。因此，作者提出Part-of-Speech enhanced image-text matching model（POS-SCAN）作为一种知识提取方法来规范模型的注意力，为模型提供一种弱的定位监督信息。所谓的“弱”是由于POS-SCAN只依赖于图片-文本对齐，而不需要昂贵的单词-区域对齐。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Image-Text Matching" scheme="https://ezeli.github.io/tags/Image-Text-Matching/"/>
    
  </entry>
  
  <entry>
    <title>Mining Ground Truth Information for Image Captioning</title>
    <link href="https://ezeli.github.io/2020/09/13/Mining-Ground-Truth-Information-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/09/13/Mining-Ground-Truth-Information-for-Image-Captioning/</id>
    <published>2020-09-13T12:23:53.000Z</published>
    <updated>2020-09-13T12:46:25.422Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-generating-diverse-and-descriptive-image-captions-using-visual-paraphrases-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-generating-diverse-and-descriptive-image-captions-using-visual-paraphrases-iccv2019&quot;&gt;&lt;/a&gt; 一、Generating Diverse and Descriptive Image Captions Using Visual Paraphrases, ICCV2019&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;计算机更喜欢生成流畅正确但是简单模糊的描述，因为这样的描述更加“安全”，能够描述图片中显著的区域，但是会忽略细节。描述相同图片的不同的句子被称为visual paraphrases，之前的方法会忽略它们之间的联系，直接把它们作为不同的样本，而作者探索了它们之间的关系并使用一系列打分函数选择了一些visual paraphrase对（Ci，Cj），打分函数衡量了visual paraphrases在某种特征（比如多样性）上的差异，而Cj在这种特征上比Ci更“复杂”，比如Cj比Ci更丰富，训练时，模型首先更加视觉特征生成初始描述Ci，之后再融合视觉特征和Ci提供的文本特征生成更加丰富多样的Cj。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/07/12/Transformer-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/07/12/Transformer-Related-Image-Captioning/</id>
    <published>2020-07-12T15:32:10.000Z</published>
    <updated>2020-07-12T15:55:23.275Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-entangled-transformer-for-image-captioning-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-entangled-transformer-for-image-captioning-iccv2019&quot;&gt;&lt;/a&gt; 一、Entangled Transformer for Image Captioning, ICCV2019&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;之前的注意力机制主要分为两种：视觉注意力和语义注意力，对于视觉注意力来说，能够探索图片底层的特征或者高层的显著对象特征，但是由于视觉和语言之间存在差异，描述中不是每个词都有对应的视觉信号，特别是对于一些抽象概念词和复杂的关系词。对于语义注意力来说，能够直接利用高层的语义信息，但是由于RNN的长期依赖问题，很难记忆很多步之前的输入信息，尤其是最初的视觉输入，导致模型倾向于生成一些高频短语而不考虑视觉信息。并且之前的方法要不只考虑一种注意力，要不分开考虑两种注意力然后在进行融合，这样不能同时探索两种注意力之间的互补关系。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (3)</title>
    <link href="https://ezeli.github.io/2020/04/26/Attention-Related-Image-Captioning-3/"/>
    <id>https://ezeli.github.io/2020/04/26/Attention-Related-Image-Captioning-3/</id>
    <published>2020-04-26T14:50:11.000Z</published>
    <updated>2020-04-26T15:03:14.713Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-hierarchical-attention-network-for-image-captioning-aaai-2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-hierarchical-attention-network-for-image-captioning-aaai-2019&quot;&gt;&lt;/a&gt; 一、Hierarchical Attention Network for Image Captioning, AAAI 2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2019%20Hierarchical%20Attention%20Network%20for%20Image%20Captioning.ppt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;这篇论文认为现有的注意力机制只关注单层特征，比如低层的空间特征或者高层的文本特征，但是描述语句中不同的单词和不同层次的特征有关，比如颜色单词可以从低层特征预测，量词可以从中层特征预测。所以作者提出了一个&lt;br&gt;
Hierarchical Attention Network (HAN)，能够同时考虑多层特征，如下图所示：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>Improve Optimization Method for Image Captioning</title>
    <link href="https://ezeli.github.io/2020/04/19/Improve-Optimization-Method-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/04/19/Improve-Optimization-Method-for-Image-Captioning/</id>
    <published>2020-04-19T14:27:04.000Z</published>
    <updated>2020-04-20T08:26:29.523Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-improving-image-captioning-with-conditional-generative-adversarial-nets-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-improving-image-captioning-with-conditional-generative-adversarial-nets-aaai2019&quot;&gt;&lt;/a&gt; 一、Improving Image Captioning with Conditional Generative Adversarial Nets, AAAI2019&lt;/h2&gt;
&lt;p&gt;这篇论文的思路很简单，就是引入GAN来提高模型生成描述的质量，这种方法是通用的，可以加入到所有的基于强化学习的模型中。模型的整体框架如下所示：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Cross-domain Image Captioning</title>
    <link href="https://ezeli.github.io/2020/04/12/Cross-domain-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/04/12/Cross-domain-Image-Captioning/</id>
    <published>2020-04-12T12:11:43.000Z</published>
    <updated>2020-04-20T08:26:29.513Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在配对的图片-文本描述数据集（下面称为源域）上，描述生成模型已经取得了不错的效果，但是由于配对数据获取是费时费力的，代价昂贵，所以产生了跨域图片文本描述任务，其目的是利用源域数据，给未配对的数据集（下面称为目标域）生成图片描述，但是由于不同的数据集之间存在域偏移，直接使用在源域上训练好的模型给目标域图片生成的描述，质量往往很差，此时，就可以使用迁移学习方法解决该问题。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Transfer Learning" scheme="https://ezeli.github.io/tags/Transfer-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/04/06/Attention-Related-Image-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/04/06/Attention-Related-Image-Captioning-2/</id>
    <published>2020-04-06T10:58:57.000Z</published>
    <updated>2020-04-20T08:26:29.495Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-reflective-decoding-network-for-image-captioning-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-reflective-decoding-network-for-image-captioning-iccv2019&quot;&gt;&lt;/a&gt; 一、Reflective Decoding Network for Image Captioning, ICCV2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ICCV2019%20Reflective%20Decoding%20Network%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;这篇论文认为当图片的内容比较复杂时传统的LSTM很难处理长期依赖问题，往往更关注于相对比较近的单词而忽略比较远的问题，如下图所示，“bridge”对“river”有很大的提示作用，但是它们之间相差6个单词，对于传统的LSTM来说很容易忽略这个。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/03/29/Attention-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/03/29/Attention-Related-Image-Captioning/</id>
    <published>2020-03-29T12:42:45.000Z</published>
    <updated>2020-04-21T04:30:32.397Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-show-attend-and-tell-neural-image-caption-generation-with-visual-attention-icml2015&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-show-attend-and-tell-neural-image-caption-generation-with-visual-attention-icml2015&quot;&gt;&lt;/a&gt; 一、Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML2015&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;我们人类观察图片时，不是将整个图片压缩成一个静态表示，而是通过注意力机制根据需要动态调节所观察的区域，当图像中存在大量杂乱时，这一点尤其重要。因此，作者参考人类的视觉系统，在编解码框架的基础上引入了注意力机制，使得模型在生成描述时可以动态关注图片的不同区域。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>(ICCV2019) Towards Unsupervised Image Captioning with Shared Multimodal Embeddings</title>
    <link href="https://ezeli.github.io/2019/10/01/ICCV2019-Towards-Unsupervised-Image-Captioning-with-Shared-Multimodal-Embeddings/"/>
    <id>https://ezeli.github.io/2019/10/01/ICCV2019-Towards-Unsupervised-Image-Captioning-with-Shared-Multimodal-Embeddings/</id>
    <published>2019-10-01T14:45:04.000Z</published>
    <updated>2020-04-20T08:26:29.474Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.09317&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文链接&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;一-主要思想&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-主要思想&quot;&gt;&lt;/a&gt; 一、主要思想&lt;/h2&gt;
&lt;p&gt;文章首先提出了传统方法的问题：给图片标注描述信息很繁琐、昂贵，并且这些人工描述也都很短而且是重复的；当前的方法生成的描述受限于图片类别数和并不完美的验证指标，因此很难扩展到数据集以外的图片中。本文使用无监督的方式对独立的图像和文本进行建模，避免了带有人为偏见的而且昂贵的人工标注，当然，语料和图片并不是完全无关的，比如试图用经济学文本语料来描述图片是不合理的。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="DL" scheme="https://ezeli.github.io/tags/DL/"/>
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Seq2Seq" scheme="https://ezeli.github.io/tags/Seq2Seq/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>(CVPR2019) Unsupervised Image Captioning</title>
    <link href="https://ezeli.github.io/2019/09/20/CVPR2019-Unsupervised-Image-Captioning/"/>
    <id>https://ezeli.github.io/2019/09/20/CVPR2019-Unsupervised-Image-Captioning/</id>
    <published>2019-09-20T13:28:31.000Z</published>
    <updated>2020-04-21T04:07:53.428Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.10787&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文链接&lt;/a&gt; | &lt;a href=&quot;https://github.com/fengyang0317/unsupervised_captioning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码链接&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;一-主要思想&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-主要思想&quot;&gt;&lt;/a&gt; 一、主要思想&lt;/h2&gt;
&lt;p&gt;作者首先总结了以往的Image Caption的方法，将这些方法分成以下几类：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="DL" scheme="https://ezeli.github.io/tags/DL/"/>
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>五、三维图形变换原理和实践</title>
    <link href="https://ezeli.github.io/2019/08/26/%E4%BA%94%E3%80%81%E4%B8%89%E7%BB%B4%E5%9B%BE%E5%BD%A2%E5%8F%98%E6%8D%A2%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>https://ezeli.github.io/2019/08/26/五、三维图形变换原理和实践/</id>
    <published>2019-08-26T08:01:07.000Z</published>
    <updated>2020-09-13T13:12:37.566Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-原理&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-原理&quot;&gt;&lt;/a&gt; 1、原理&lt;/h2&gt;
&lt;h3 id=&quot;1四种坐标系&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1四种坐标系&quot;&gt;&lt;/a&gt; 1）四种坐标系&lt;/h3&gt;
&lt;p&gt;为了将三维立体图形投影到平面上，应该首先理解以下四种坐标系：&lt;/p&gt;
&lt;p&gt;本体坐标系（模型坐标系）：是为规定基本形体而引入的便于描述的坐标系，也就是立体图形自身的坐标系。&lt;/p&gt;
&lt;p&gt;用户坐标系（世界坐标系）：是用户引入描述整个形体的坐标系。&lt;/p&gt;
&lt;p&gt;观察坐标系（视坐标系或目坐标系）：为说明观察的姿态而引入，也就是观察者所处的位置。&lt;/p&gt;
&lt;p&gt;设备坐标系（屏幕坐标系或显示坐标系）：最终的图形显示设备的坐标系。&lt;/p&gt;
    
    </summary>
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>四、多边形的扫描转换算法原理和实践</title>
    <link href="https://ezeli.github.io/2019/08/26/%E5%9B%9B%E3%80%81%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>https://ezeli.github.io/2019/08/26/四、多边形的扫描转换算法原理和实践/</id>
    <published>2019-08-26T07:59:40.000Z</published>
    <updated>2020-09-13T13:13:49.556Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-原理&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-原理&quot;&gt;&lt;/a&gt; 1、原理&lt;/h2&gt;
&lt;h3 id=&quot;1区域的奇偶性质&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1区域的奇偶性质&quot;&gt;&lt;/a&gt; 1）区域的“奇偶”性质&lt;/h3&gt;
&lt;p&gt;多边形扫描转换主要依据区域的一种“奇偶”性质，即一条直线与任意封闭的曲线相交时，总是从第一个交点进入内部，再从第二个交点退出，在交替的进入退出过程中对多边形进行填充。&lt;/p&gt;
    
    </summary>
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>三、种子填充算法原理和实践</title>
    <link href="https://ezeli.github.io/2019/08/26/%E4%B8%89%E3%80%81%E7%A7%8D%E5%AD%90%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>https://ezeli.github.io/2019/08/26/三、种子填充算法原理和实践/</id>
    <published>2019-08-26T07:58:11.000Z</published>
    <updated>2020-10-27T06:49:47.163Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;主要内容&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#主要内容&quot;&gt;&lt;/a&gt; 主要内容&lt;/h2&gt;
&lt;p&gt;种子填充算法&lt;/p&gt;
    
    </summary>
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>二、Bresenham画圆算法原理和实践</title>
    <link href="https://ezeli.github.io/2019/08/26/%E4%BA%8C%E3%80%81Bresenham%E7%94%BB%E5%9C%86%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>https://ezeli.github.io/2019/08/26/二、Bresenham画圆算法原理和实践/</id>
    <published>2019-08-26T07:41:36.000Z</published>
    <updated>2019-08-26T07:57:36.336Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;主要内容&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#主要内容&quot;&gt;&lt;/a&gt; 主要内容&lt;/h2&gt;
&lt;p&gt;Bresenham画圆算法&lt;/p&gt;
    
    </summary>
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>一、DDA直线扫描转换算法原理和实践</title>
    <link href="https://ezeli.github.io/2019/08/26/%E4%B8%80%E3%80%81DDA%E7%9B%B4%E7%BA%BF%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>https://ezeli.github.io/2019/08/26/一、DDA直线扫描转换算法原理和实践/</id>
    <published>2019-08-26T07:39:22.000Z</published>
    <updated>2019-08-26T07:57:46.312Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;主要内容&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#主要内容&quot;&gt;&lt;/a&gt; 主要内容&lt;/h2&gt;
&lt;p&gt;DDA直线扫描转换算法&lt;/p&gt;
    
    </summary>
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
      <category term="计算机图形学" scheme="https://ezeli.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
</feed>
