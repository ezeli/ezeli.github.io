<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ezeli&#39;s Blog</title>
  
  <subtitle>人生在勤，不索何获？</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ezeli.github.io/"/>
  <updated>2021-02-28T14:51:59.135Z</updated>
  <id>https://ezeli.github.io/</id>
  
  <author>
    <name>Ezeli</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Video Grounding</title>
    <link href="https://ezeli.github.io/2021/02/28/Video-Grounding/"/>
    <id>https://ezeli.github.io/2021/02/28/Video-Grounding/</id>
    <published>2021-02-28T14:49:34.000Z</published>
    <updated>2021-02-28T14:51:59.135Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-dense-regression-network-for-video-grounding-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-dense-regression-network-for-video-grounding-cvpr2020&quot;&gt;&lt;/a&gt; 一、Dense Regression Network for Video Grounding, CVPR2020&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;如下图所示，Video Grounding任务的目的是在视频中定位对应于给定查询（一句描述）的目标视频段的起始和结束时间，该任务一个关键问题是视频通常可以包含数千帧，但它可能只有很少的帧被标注为起始和结束帧（即正训练示例），而之前的方法都忽略了来自被标注的起始结束位置之间的帧的丰富信息，往往直接使用这些不平衡的数据训练一个二分类模型。因此，作者训练了一个模型来预测每个帧到目标视频段边界的距离，并将目标视频段范围内的所有帧都作为正训练样本，这样训练样本大大增加，有利于训练。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/02/28/Video-Grounding/78da75a489c3fc4ea89236cc822d8ce7.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Video Grounding" scheme="https://ezeli.github.io/tags/Video-Grounding/"/>
    
  </entry>
  
  <entry>
    <title>News Image Captioning</title>
    <link href="https://ezeli.github.io/2021/02/28/News-Image-Captioning/"/>
    <id>https://ezeli.github.io/2021/02/28/News-Image-Captioning/</id>
    <published>2021-02-28T14:38:32.000Z</published>
    <updated>2021-02-28T14:41:53.276Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-transform-and-tell-entity-aware-news-image-captioning-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-transform-and-tell-entity-aware-news-image-captioning-cvpr2020&quot;&gt;&lt;/a&gt; 一、Transform and Tell: Entity-Aware News Image Captioning, CVPR2020&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;新闻中图片描述生成任务主要有两个挑战：一是依赖于现实世界的知识，特别是关于命名实体的知识；二是新闻中有丰富的语言描述，它们包括一些不常见的单词。作者通过一种多模态、多头注意机制将新闻中的单词与图像中的人脸和对象关联起来来解决第一个挑战，并且用Transformer语言模型来解决第二个挑战，该模型使用byte-pair-encoding（BPE）来编码单词：首先将单词表示为字母序列，之后使用贪婪算法合并公共字母序列，这样几乎所有的单词都可以用字母序列的组合来表示，这样就可以处理一些新的不常见的单词，而不需要使用一个特殊的未知标签代替。并且作者提出一个新的大型新闻图片描述数据集：NYTimes800k，包含445K文章和793K图片。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-Related Image Captioning (2)</title>
    <link href="https://ezeli.github.io/2021/02/08/Transformer-Related-Image-Captioning-2/"/>
    <id>https://ezeli.github.io/2021/02/08/Transformer-Related-Image-Captioning-2/</id>
    <published>2021-02-08T10:50:43.000Z</published>
    <updated>2021-02-08T11:09:53.928Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-normalized-and-geometry-aware-self-attention-network-for-image-captioning-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-normalized-and-geometry-aware-self-attention-network-for-image-captioning-cvpr2020&quot;&gt;&lt;/a&gt; 一、Normalized and Geometry-Aware Self-Attention Network for Image Captioning, CVPR2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2020%20Normalized%20and%20Geometry-Aware%20Self-Attention%20Network%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;随着Transformer在NLP领域的流行，它的self-attention（SA）的思想也逐渐被引入到image captioning领域。但是原始的SA有两个问题：&lt;/p&gt;
&lt;p&gt;1）Internal Covariate Shift（ICS）问题：在训练过程中，由于网络参数的变化，当query的分布发生变化时，该层的输出的分布会发生变化，也就是说，随后的层必须不断适应新的输入分布，因此，SA可能无法有效地学习。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (4)</title>
    <link href="https://ezeli.github.io/2021/02/01/Attention-Related-Image-Captioning-4/"/>
    <id>https://ezeli.github.io/2021/02/01/Attention-Related-Image-Captioning-4/</id>
    <published>2021-02-01T12:40:32.000Z</published>
    <updated>2021-02-01T12:44:04.843Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-exploring-and-distilling-cross-modal-information-for-image-captioning-ijcai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-exploring-and-distilling-cross-modal-information-for-image-captioning-ijcai2019&quot;&gt;&lt;/a&gt; 一、Exploring and Distilling Cross-Modal Information for Image Captioning, IJCAI2019&lt;/h2&gt;
&lt;p&gt;作者认为深层次的图片理解需要与图片区域相关的视觉注意力和与对应属性相关的语义注意力，所以作者从跨模态（包括视觉和文本属性）的视角提出了Global-and-Local Information Exploring-and-Distilling (GLIED) 方法，如下图所示，左边是基础模型，右边是利用全局和局部的源信息的完整模型。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>元学习应用论文</title>
    <link href="https://ezeli.github.io/2021/01/27/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E8%AE%BA%E6%96%87/"/>
    <id>https://ezeli.github.io/2021/01/27/元学习应用论文/</id>
    <published>2021-01-27T14:36:15.000Z</published>
    <updated>2021-01-27T14:41:48.839Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-meta-learning-for-image-captioning-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-meta-learning-for-image-captioning-aaai2019&quot;&gt;&lt;/a&gt; 一、Meta Learning for Image Captioning, AAAI2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2019%20Meta%20Learning%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;使用强化学习训练模型，可能会使模型与奖励函数过拟合，造成奖励黑客行为（reward hacking），也就是说虽然奖励函数的得分提高了但是模型的实际性能是降低的，这是因为奖励函数不能非常正确的表达出生成描述的质量，一些错误的表示可能反而有很高的奖励，尤其是对描述的命题内容和独特性的奖励。比如，使用CIDEr优化倾向于让句子以“介词+a”结尾，因为CIDEr评价指标会惩罚太短的句子并给常见的短语小的权重，因此，当模型生成短描述时RL会添加一些小权重但是常见的短语来避免惩罚。SPICE指标认为不正常结尾是不匹配的对象-关系对，会惩罚这种现象，但是，SPICE有自己的奖励黑客问题，因为它不惩罚场景图中的重复元组。从技术上讲，很难设计一个完美的评价指标，能够考虑到预期目标的每一个方面。&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
      <category term="Face Recognition" scheme="https://ezeli.github.io/tags/Face-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Active Learning for Image Captioning</title>
    <link href="https://ezeli.github.io/2021/01/04/Active-Learning-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2021/01/04/Active-Learning-for-Image-Captioning/</id>
    <published>2021-01-04T02:38:20.000Z</published>
    <updated>2021-01-04T02:46:57.745Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-structural-semantic-adversarial-active-learning-for-image-captioning-acmmm2020-oral&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-structural-semantic-adversarial-active-learning-for-image-captioning-acmmm2020-oral&quot;&gt;&lt;/a&gt; 一、Structural Semantic Adversarial Active Learning for Image Captioning, ACMMM2020 oral&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ACMMM2020%20oral%20Structural%20Semantic%20Adversarial%20Active%20Learning%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;标注图片描述的成本非常高。&lt;/p&gt;
&lt;h3 id=&quot;2-方法&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#2-方法&quot;&gt;&lt;/a&gt; 2、方法&lt;/h3&gt;
&lt;h4 id=&quot;1active-learning&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1active-learning&quot;&gt;&lt;/a&gt; 1）Active learning&lt;/h4&gt;
&lt;p&gt;引入主动学习（Active learning）方法来从未标注的图片集中选取最具代表性的图片来进行标注，再用于训练captioning模型。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Active Learning" scheme="https://ezeli.github.io/tags/Active-Learning/"/>
    
  </entry>
  
  <entry>
    <title>元学习基础论文</title>
    <link href="https://ezeli.github.io/2020/12/28/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E8%AE%BA%E6%96%87/"/>
    <id>https://ezeli.github.io/2020/12/28/元学习基础论文/</id>
    <published>2020-12-28T07:46:05.000Z</published>
    <updated>2021-02-24T02:11:13.104Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-learning-to-learn-by-gradient-descent-by-gradient-descent-nips2016&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-learning-to-learn-by-gradient-descent-by-gradient-descent-nips2016&quot;&gt;&lt;/a&gt; 一、Learning to learn by gradient descent by gradient descent, NIPS2016&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;在机器学习中，学习得到的特征取代了手工设计的特征，并取得了巨大的成功。但是，优化算法依然是手工设计的，针对不同的任务需要设计不同的更新规则，比如深度学习中常用的momentum、RMSprop、ADAM等。这篇论文中，作者将优化算法参数化，使用LSTM来作为优化器，通过学习的方式得到更新规则，也就是论文题目：通过梯度下降来学习如何通过梯度下降学习。这样就不需要针对不同的任务设计不同的优化算法，而只需要让LSTM优化器自己学习如何优化模型的参数即可。&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Image Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/12/16/Detailed-Image-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/12/16/Detailed-Image-Captioning-2/</id>
    <published>2020-12-16T13:20:09.000Z</published>
    <updated>2020-12-16T13:39:56.784Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-say-as-you-wish-fine-grained-control-of-image-caption-generation-with-abstract-scene-graphs-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-say-as-you-wish-fine-grained-control-of-image-caption-generation-with-abstract-scene-graphs-cvpr2020&quot;&gt;&lt;/a&gt; 一、Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs, CVPR2020&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;现有的方法无法根据用户的意图细粒度的控制生成的描述，比如控制以什么层次的细节程度来描述图片中的物体、以及那些属性和关系应该包含在描述中等等。只存在一些粗粒度的控制，比如控制描述的风格、描述哪个物体、描述图片那个区域等。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
  </entry>
  
  <entry>
    <title>Stylized Image Captioning</title>
    <link href="https://ezeli.github.io/2020/11/02/Stylized-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/11/02/Stylized-Image-Captioning/</id>
    <published>2020-11-02T01:05:57.000Z</published>
    <updated>2020-11-02T01:16:23.812Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-unsupervised-stylish-image-description-generation-via-domain-layer-norm-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-unsupervised-stylish-image-description-generation-via-domain-layer-norm-aaai2019&quot;&gt;&lt;/a&gt; 一、Unsupervised Stylish Image Description Generation via Domain Layer Norm, AAAI2019&lt;/h2&gt;
&lt;p&gt;作者提出了一种无监督风格化描述生成模型，能够以配对的无风格数据和没有配对的风格化语料进行训练，并且它使用户能够通过插入特定样式的参数来生成各种风格描述，灵活地将新的样式包含到现有模型中。论文将配对的无风格描述视为源域数据，将未配对的风格化语料视为目标域数据，最主要的贡献就是展示了只需要对layer normalization的参数进行调整就能从源域和目标域区分出语言风格，并将这种机制称为Domain Layer Normalization (DLN)，结构图如下：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
      <category term="Stylized Image Captioning" scheme="https://ezeli.github.io/tags/Stylized-Image-Captioning/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Image Captioning</title>
    <link href="https://ezeli.github.io/2020/10/18/Detailed-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/10/18/Detailed-Image-Captioning/</id>
    <published>2020-10-18T07:28:03.000Z</published>
    <updated>2020-10-18T07:36:50.260Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-compare-and-reweight-distinctive-image-captioning-using-similar-images-sets-eccv2020-oral&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-compare-and-reweight-distinctive-image-captioning-using-similar-images-sets-eccv2020-oral&quot;&gt;&lt;/a&gt; 一、Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets, ECCV2020 oral&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ECCV2020%20oral%20Compare%20and%20Reweight%20Distinctive%20Image%20Captioning%20Using%20Similar%20Images%20Sets.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;1）现在的方法生成的句子可以准确的描述图片，但是对于相似的图片，生成的句子是通用的，缺乏独特性。如下图所示，CIDErBtw是作者提出的一种衡量描述独特性的指标，值越小表示越独特，对于两个相似的图片，人们标注的描述具有很好的独特性，但是baseline模型生成的描述就是相同的，而作者的方法生成的描述则具有不错的独特性。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="RL" scheme="https://ezeli.github.io/tags/RL/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>元学习基础知识</title>
    <link href="https://ezeli.github.io/2020/10/13/%E5%85%83%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>https://ezeli.github.io/2020/10/13/元学习基础知识/</id>
    <published>2020-10-13T07:40:01.000Z</published>
    <updated>2020-10-13T07:56:13.399Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;From &lt;a href=&quot;https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;amp;index=32&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2019%20Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;introduction&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#introduction&quot;&gt;&lt;/a&gt; Introduction&lt;/h2&gt;
&lt;p&gt;Meta learning = Learn to learn&lt;/p&gt;
    
    </summary>
    
      <category term="元学习" scheme="https://ezeli.github.io/categories/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Meta learning" scheme="https://ezeli.github.io/tags/Meta-learning/"/>
    
  </entry>
  
  <entry>
    <title>Scene Graph-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/10/04/Scene-Graph-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/10/04/Scene-Graph-Related-Image-Captioning/</id>
    <published>2020-10-04T15:37:35.000Z</published>
    <updated>2020-10-04T15:57:31.950Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-auto-encoding-scene-graphs-for-image-captioning-cvpr2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-auto-encoding-scene-graphs-for-image-captioning-cvpr2019&quot;&gt;&lt;/a&gt; 一、Auto-Encoding Scene Graphs for Image Captioning, CVPR2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2019%20Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;当我们看到“person on bike”，我们会很自然的把“on”替换成“ride”并且推理出“person riding bike on a road”即使“road”并没有出现，我们人类能够通过这种inductive bias（归纳能力、常识、先验知识）来进行单词搭配和语境推理。因此，探索这种推理可以让模型不过度拟合于数据集偏差而专注于推理。之前的工作中，当我们将一个看不见的图像场景输入到模型中时，通常会得到一个简单而没有价值的关于显著物体的标题，例如“there is a dog on the floor”，这比目标检测得到的结果好不了多少。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Scene Graph" scheme="https://ezeli.github.io/tags/Scene-Graph/"/>
    
      <category term="Unsupervised Learning" scheme="https://ezeli.github.io/tags/Unsupervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improve Optimization Method for Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/09/27/Improve-Optimization-Method-for-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/09/27/Improve-Optimization-Method-for-Captioning-2/</id>
    <published>2020-09-27T12:37:21.000Z</published>
    <updated>2020-09-27T13:06:33.568Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-reinforcing-an-image-caption-generator-using-off-line-human-feedback-aaai2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-reinforcing-an-image-caption-generator-using-off-line-human-feedback-aaai2020&quot;&gt;&lt;/a&gt; 一、Reinforcing an Image Caption Generator Using Off-Line Human Feedback, AAAI2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2020%20Reinforcing%20an%20Image%20Caption%20Generator%20Using%20Off-Line%20Human%20Feedback.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;之前模型主要是通过最大似然估计（MLE）以及使用CIDEr等手工设计的评价指标作为奖励函数的强化学习的方式进行优化，但是这些优化方法是受限的，我们人类对模型生成描述的质量评估可能并不高。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Video Captioning" scheme="https://ezeli.github.io/tags/Video-Captioning/"/>
    
  </entry>
  
  <entry>
    <title>Image Captioning with Image-Text Matching Model</title>
    <link href="https://ezeli.github.io/2020/09/21/Image-Captioning-with-Image-Text-Matching-Model/"/>
    <id>https://ezeli.github.io/2020/09/21/Image-Captioning-with-Image-Text-Matching-Model/</id>
    <published>2020-09-21T11:47:07.000Z</published>
    <updated>2020-09-27T12:46:48.997Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-more-grounded-image-captioning-by-distilling-image-text-matching-model-cvpr2020&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-more-grounded-image-captioning-by-distilling-image-text-matching-model-cvpr2020&quot;&gt;&lt;/a&gt; 一、More Grounded Image Captioning by Distilling Image-Text Matching Model, CVPR2020&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/CVPR2020%20More%20Grounded%20Image%20Captioning%20by%20Distilling%20Image-Text%20Matching%20Model.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;注意力机制的目的是为了让模型在生成对应单词时将注意力集中到正确的物体上，这种能力被称为grounded image captioning，但是现有模型的定位精度远远不能令人满意，并且如果为了提高定位精度而收集单词-区域对齐数据作为强监督信息，代价是很昂贵的。因此，作者提出Part-of-Speech enhanced image-text matching model（POS-SCAN）作为一种知识提取方法来规范模型的注意力，为模型提供一种弱的定位监督信息。所谓的“弱”是由于POS-SCAN只依赖于图片-文本对齐，而不需要昂贵的单词-区域对齐。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Image-Text Matching" scheme="https://ezeli.github.io/tags/Image-Text-Matching/"/>
    
  </entry>
  
  <entry>
    <title>Mining Ground Truth Information for Image Captioning</title>
    <link href="https://ezeli.github.io/2020/09/13/Mining-Ground-Truth-Information-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/09/13/Mining-Ground-Truth-Information-for-Image-Captioning/</id>
    <published>2020-09-13T12:23:53.000Z</published>
    <updated>2020-09-13T12:46:25.422Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-generating-diverse-and-descriptive-image-captions-using-visual-paraphrases-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-generating-diverse-and-descriptive-image-captions-using-visual-paraphrases-iccv2019&quot;&gt;&lt;/a&gt; 一、Generating Diverse and Descriptive Image Captions Using Visual Paraphrases, ICCV2019&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;计算机更喜欢生成流畅正确但是简单模糊的描述，因为这样的描述更加“安全”，能够描述图片中显著的区域，但是会忽略细节。描述相同图片的不同的句子被称为visual paraphrases，之前的方法会忽略它们之间的联系，直接把它们作为不同的样本，而作者探索了它们之间的关系并使用一系列打分函数选择了一些visual paraphrase对（Ci，Cj），打分函数衡量了visual paraphrases在某种特征（比如多样性）上的差异，而Cj在这种特征上比Ci更“复杂”，比如Cj比Ci更丰富，训练时，模型首先更加视觉特征生成初始描述Ci，之后再融合视觉特征和Ci提供的文本特征生成更加丰富多样的Cj。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-Related Image Captioning</title>
    <link href="https://ezeli.github.io/2020/07/12/Transformer-Related-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/07/12/Transformer-Related-Image-Captioning/</id>
    <published>2020-07-12T15:32:10.000Z</published>
    <updated>2020-07-12T15:55:23.275Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-entangled-transformer-for-image-captioning-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-entangled-transformer-for-image-captioning-iccv2019&quot;&gt;&lt;/a&gt; 一、Entangled Transformer for Image Captioning, ICCV2019&lt;/h2&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;之前的注意力机制主要分为两种：视觉注意力和语义注意力，对于视觉注意力来说，能够探索图片底层的特征或者高层的显著对象特征，但是由于视觉和语言之间存在差异，描述中不是每个词都有对应的视觉信号，特别是对于一些抽象概念词和复杂的关系词。对于语义注意力来说，能够直接利用高层的语义信息，但是由于RNN的长期依赖问题，很难记忆很多步之前的输入信息，尤其是最初的视觉输入，导致模型倾向于生成一些高频短语而不考虑视觉信息。并且之前的方法要不只考虑一种注意力，要不分开考虑两种注意力然后在进行融合，这样不能同时探索两种注意力之间的互补关系。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
      <category term="Transformer" scheme="https://ezeli.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (3)</title>
    <link href="https://ezeli.github.io/2020/04/26/Attention-Related-Image-Captioning-3/"/>
    <id>https://ezeli.github.io/2020/04/26/Attention-Related-Image-Captioning-3/</id>
    <published>2020-04-26T14:50:11.000Z</published>
    <updated>2020-04-26T15:03:14.713Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-hierarchical-attention-network-for-image-captioning-aaai-2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-hierarchical-attention-network-for-image-captioning-aaai-2019&quot;&gt;&lt;/a&gt; 一、Hierarchical Attention Network for Image Captioning, AAAI 2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/AAAI2019%20Hierarchical%20Attention%20Network%20for%20Image%20Captioning.ppt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;这篇论文认为现有的注意力机制只关注单层特征，比如低层的空间特征或者高层的文本特征，但是描述语句中不同的单词和不同层次的特征有关，比如颜色单词可以从低层特征预测，量词可以从中层特征预测。所以作者提出了一个&lt;br&gt;
Hierarchical Attention Network (HAN)，能够同时考虑多层特征，如下图所示：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
  <entry>
    <title>Improve Optimization Method for Image Captioning</title>
    <link href="https://ezeli.github.io/2020/04/19/Improve-Optimization-Method-for-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/04/19/Improve-Optimization-Method-for-Image-Captioning/</id>
    <published>2020-04-19T14:27:04.000Z</published>
    <updated>2020-04-20T08:26:29.523Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-improving-image-captioning-with-conditional-generative-adversarial-nets-aaai2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-improving-image-captioning-with-conditional-generative-adversarial-nets-aaai2019&quot;&gt;&lt;/a&gt; 一、Improving Image Captioning with Conditional Generative Adversarial Nets, AAAI2019&lt;/h2&gt;
&lt;p&gt;这篇论文的思路很简单，就是引入GAN来提高模型生成描述的质量，这种方法是通用的，可以加入到所有的基于强化学习的模型中。模型的整体框架如下所示：&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="GAN" scheme="https://ezeli.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Cross-domain Image Captioning</title>
    <link href="https://ezeli.github.io/2020/04/12/Cross-domain-Image-Captioning/"/>
    <id>https://ezeli.github.io/2020/04/12/Cross-domain-Image-Captioning/</id>
    <published>2020-04-12T12:11:43.000Z</published>
    <updated>2020-04-20T08:26:29.513Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在配对的图片-文本描述数据集（下面称为源域）上，描述生成模型已经取得了不错的效果，但是由于配对数据获取是费时费力的，代价昂贵，所以产生了跨域图片文本描述任务，其目的是利用源域数据，给未配对的数据集（下面称为目标域）生成图片描述，但是由于不同的数据集之间存在域偏移，直接使用在源域上训练好的模型给目标域图片生成的描述，质量往往很差，此时，就可以使用迁移学习方法解决该问题。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Transfer Learning" scheme="https://ezeli.github.io/tags/Transfer-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Related Image Captioning (2)</title>
    <link href="https://ezeli.github.io/2020/04/06/Attention-Related-Image-Captioning-2/"/>
    <id>https://ezeli.github.io/2020/04/06/Attention-Related-Image-Captioning-2/</id>
    <published>2020-04-06T10:58:57.000Z</published>
    <updated>2020-04-20T08:26:29.495Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-reflective-decoding-network-for-image-captioning-iccv2019&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#一-reflective-decoding-network-for-image-captioning-iccv2019&quot;&gt;&lt;/a&gt; 一、Reflective Decoding Network for Image Captioning, ICCV2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ezeli/notes_in_BIT/raw/master/PPT/ICCV2019%20Reflective%20Decoding%20Network%20for%20Image%20Captioning.pptx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自制PPT&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-解决问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-解决问题&quot;&gt;&lt;/a&gt; 1、解决问题&lt;/h3&gt;
&lt;p&gt;这篇论文认为当图片的内容比较复杂时传统的LSTM很难处理长期依赖问题，往往更关注于相对比较近的单词而忽略比较远的问题，如下图所示，“bridge”对“river”有很大的提示作用，但是它们之间相差6个单词，对于传统的LSTM来说很容易忽略这个。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://ezeli.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="Image Caption" scheme="https://ezeli.github.io/tags/Image-Caption/"/>
    
      <category term="Attention Mechanism" scheme="https://ezeli.github.io/tags/Attention-Mechanism/"/>
    
  </entry>
  
</feed>
